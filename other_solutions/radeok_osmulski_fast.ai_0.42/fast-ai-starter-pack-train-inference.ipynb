{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca81b72c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.009205,
     "end_time": "2022-12-11T23:15:56.089280",
     "exception": false,
     "start_time": "2022-12-11T23:15:56.080075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook contains the complete, end to end pipeline for training and successfully predicting on test!\n",
    "\n",
    "Here are the things we'll cover.\n",
    "\n",
    "1. [Training a fast.ai model](#section-one)\n",
    "2. [Using the model to predict on test](#section-two)\n",
    "3. [Making a submission](#section-three)\n",
    "\n",
    "For training, we will use images preprocessed to PNGs that I shared here: [RSNA Mammo PNGs 256px, 384px, 512px, 768px, 1024px](https://www.kaggle.com/datasets/radek1/rsna-mammography-images-as-pngs)\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c547753d",
   "metadata": {
    "papermill": {
     "duration": 0.005031,
     "end_time": "2022-12-11T23:15:56.102834",
     "exception": false,
     "start_time": "2022-12-11T23:15:56.097803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Other resources you might find useful:\n",
    "\n",
    "* [üí° how to process DICOM images to PNGs](https://www.kaggle.com/code/radek1/how-to-process-dicom-images-to-pngs)\n",
    "* [üìä EDA + training a fast.ai model + submission üöÄ](https://www.kaggle.com/code/radek1/eda-training-a-fast-ai-model-submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f7170",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.004781,
     "end_time": "2022-12-11T23:15:56.112889",
     "exception": false,
     "start_time": "2022-12-11T23:15:56.108108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Versions\n",
    "\n",
    "* **v10**: res18, 4 epochs trained in submission, 256px, 3 splits, `CV: 0.046/0.076 LB: 0.05`, unfortunately had a bug in best thresh calulation\n",
    "* **v11**: res18, 4 epochs trained in submission, 256px, 4 splits, ‚è∞ times out\n",
    "* **v15**: res18, 4 epochs trained externally, 256px, CPU (the processing of images might be the most expensive part of inference! especially with resnet18, 2 CPUs -> 4 CPUs might be the way to go)\n",
    "* **v16**: res18, 4 epochs trained externally, 256px, CPU, higher thresh (the means of predictions should be better than predictions from an individual model, a higher threshold *might* work better)\n",
    "* **v17**: res18, like `v15`, but trained with cherry picking, either performing well in general (doubt it) or overfitted to local CV\n",
    "* **v18**: tf_efficientnetv2_s, 512x512, single model\n",
    "* **v19**: tf_efficientnetv2_s, 512x512, ensemble of 4 models\n",
    "* **v20**: tf_efficientnetv2_s, like v19, but running with different pretrained weights, main difference -- this one runs on the CPU\n",
    "* **v22**: like v18, but taking max instead of mean of predictions on images, that makes more sense but who knows if it will work better in practice\n",
    "* **v23**: moving to new arch and adding VOI LUT transformation (might make the pipeline time out as it makes the processing a bit longer)\n",
    "* **v24**: like v23 but with mean of individual thresholds recorded during training\n",
    "* **v25**: like v23 but without VOI LUT processing\n",
    "* **v26**: a single model with particularly good optimized pfbeta1 of just under 0.196\n",
    "* **v27**: a single model with an optimized pfbeta1 of 0.212\n",
    "* **v28**: a single model with an optimized pfbeta1 of 0.249\n",
    "* **v30**: like v28, but with mean aggregation of predictions by `prediction_id`\n",
    "* **v31**: a single model with an optimized pfbeta1 of 0.252\n",
    "* **v32**: a single model with an optimized pfbeta1 of 0.207 trained on 1024x1024\n",
    "* **v33**: similar to v32, but trained with augmentation and results in much higher threshold (plus locally the difference between pfbeta and optimized pfbeta is much greater)\n",
    "* **v34**: like v32, but trained for one more epoch and on a different fold\n",
    "* **v35**: an ensemble of v32 and v34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a14d8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.004797,
     "end_time": "2022-12-11T23:15:56.122584",
     "exception": false,
     "start_time": "2022-12-11T23:15:56.117787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Updates (account of working on this -- documents the despair of an ML practitioner we so often encounter!)\n",
    "\n",
    "**12/04**: I find myself in the spot so often encountered by DL practitioners -- the model simply doesn't train! Or rather, it trains poorly to the point where one cannot be certain to what extent it is making good use of the available information.\n",
    "\n",
    "This is an interesting case as a lot can be happening. From what I observed on the forums (special thanks go to Theo Viel and his awesome notebook: [RSNA Breast Baseline - Inference](https://www.kaggle.com/code/theoviel/rsna-breast-baseline-inference), now there is also [this notebook](https://www.kaggle.com/code/hengck23/notebooke04a738685) by hengck23), people are feeding the data to their model in a way that I wouldn't expect. Theo is only normalizing it to between 0 and 1.\n",
    "\n",
    "But this might mean that some important information to the model is lost in the processing that I do here. Or it could be a hunderd of other small things the community has arrived at through so many RSNA competitions üôÇ.\n",
    "\n",
    "Three things to do:\n",
    "\n",
    "* explore various ways of phrsing the problem within `fastai` (training with Sigmoid, Softmax, etc)\n",
    "* get the pipeline on Kaggle working with what I have\n",
    "    * to always have a working pipeline version (otherwise it is very easy to lose momentum on a project)\n",
    "    * to be verifying if local improvements generalize to LB (so far, I have a very weak handle on what is going on in this competition, the more data I collect the better)\n",
    "* after the above, see what happens if I feed data to my model either how Theo or hengck23 does it (normalized to [0,1], zero-centered and standard dev normalized)\n",
    "\n",
    "**12/05**: I stayed up for more than half the night on Saturday and Sunday to work on this as I knew I wouldn't get much time to give this over the week. There can be an all-consuming quality to DL projects if you are not careful.\n",
    "\n",
    "I reimplemented the entire pipeline in pure PyTorch... and it doesn't train in the SAME WAY as my fast.ai pipeline...\n",
    "\n",
    "```\n",
    "step:     1    val_loss: 0.686    pf1: 0.042    auc: 0.464\n",
    "step:   500    val_loss: 0.108    pf1: 0.023    auc: 0.479\n",
    "step:  1000    val_loss: 0.106    pf1: 0.023    auc: 0.579\n",
    "step:  1500    val_loss: 0.105    pf1: 0.021    auc: 0.558\n",
    "step:  2000    val_loss: 0.107    pf1: 0.020    auc: 0.524\n",
    "step:  2500    val_loss: 0.106    pf1: 0.024    auc: 0.599\n",
    "step:  3000    val_loss: 0.105    pf1: 0.023    auc: 0.575\n",
    "step:  3500    val_loss: 0.104    pf1: 0.025    auc: 0.610\n",
    "step:  4000    val_loss: 0.103    pf1: 0.027    auc: 0.623\n",
    "step:  4500    val_loss: 0.104    pf1: 0.027    auc: 0.612\n",
    "step:  5000    val_loss: 0.104    pf1: 0.027    auc: 0.607\n",
    "step:  5125    val_loss: 0.104    pf1: 0.027    auc: 0.606\n",
    "CPU times: user 12min 15s, sys: 38 s, total: 12min 53s\n",
    "Wall time: 12min 56s\n",
    "```\n",
    "\n",
    "I replaced the training loop, how data is read, the data I am reading, added a metric from `torchmetrics` for another bit of sanity check and also did a couple of things that don't make sense but you essentially question everything when stuff doesn't train. The things I tried that didn't make much sense was training on larger images (why 512px should work if 256px doesn't train reasonably?) and training with sigmoid and BCE vs softmax and nll (with two classes they should be equivalent). Oh yeah, I also tried different models of course, directly from fast.ai, the one from Theo, pretrained, not pretrained, etc.\n",
    "\n",
    "AND YET THERE ARE PEOPLE ON THE LB whose models do seem to train, potentially with ease.\n",
    "\n",
    "What is the secret? What am I missing?\n",
    "\n",
    "The only low hanging fruit I can think of now is addressing the class imbalance... Counting by images, there are roughly 2% of positive examples in the dataset. Maybe this can make a difference.\n",
    "\n",
    "Other than that, the only 3 components I haven't tried replacing so far are:\n",
    "* the researcher working on this (me)\n",
    "* the splitting functionality into train and val\n",
    "* my computer\n",
    "* *maybe* the dataset, as in, would my code train on cats and dogs?\n",
    "\n",
    "**12/05**: I think I found a way to train the model so that it works üò≠üò≠üò≠üò≠üò≠ Don't want to jinx it though üôÇ Let me move it over to the Kaggle pipeline and let us see how it fares here üôÇ Writing [the thoughts down in this thread](https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/370587) was really helpful üôè"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c3c74c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T00:49:16.208915Z",
     "iopub.status.busy": "2022-12-02T00:49:16.208242Z",
     "iopub.status.idle": "2022-12-02T00:49:16.247884Z",
     "shell.execute_reply": "2022-12-02T00:49:16.244677Z",
     "shell.execute_reply.started": "2022-12-02T00:49:16.208760Z"
    },
    "papermill": {
     "duration": 0.004797,
     "end_time": "2022-12-11T23:15:56.132313",
     "exception": false,
     "start_time": "2022-12-11T23:15:56.127516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-one\"></a>\n",
    "# Training a fast.ai model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9514855a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:15:56.144901Z",
     "iopub.status.busy": "2022-12-11T23:15:56.143808Z",
     "iopub.status.idle": "2022-12-11T23:17:09.078614Z",
     "shell.execute_reply": "2022-12-11T23:17:09.077475Z"
    },
    "papermill": {
     "duration": 72.944243,
     "end_time": "2022-12-11T23:17:09.081513",
     "exception": false,
     "start_time": "2022-12-11T23:15:56.137270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from fastai.vision.learner import *\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.metrics import ActivationType\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pdb import set_trace\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a552b6b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:17:09.094824Z",
     "iopub.status.busy": "2022-12-11T23:17:09.094366Z",
     "iopub.status.idle": "2022-12-11T23:17:12.010025Z",
     "shell.execute_reply": "2022-12-11T23:17:12.009035Z"
    },
    "papermill": {
     "duration": 2.925095,
     "end_time": "2022-12-11T23:17:12.012470",
     "exception": false,
     "start_time": "2022-12-11T23:17:09.087375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 4\n",
    "NUM_SPLITS = 4\n",
    "\n",
    "RESIZE_TO = (1024, 1024)\n",
    "DATA_PATH = os.path.join(os.environ[\"DATASET_ROOT\"], \"bcd2022\")\n",
    "TRAIN_IMAGE_DIR = os.path.join(DATA_PATH, \"images_1024\")\n",
    "TEST_DICOM_DIR = os.path.join(DATA_PATH, \"test_images\")\n",
    "MODEL_PATH = '/kaggle/input/rsna-trained-model-weights/tf_effv2_s_208_402/tf_effv2_s_208_402'\n",
    "\n",
    "label_smoothing_weights = torch.tensor([1,10]).float()\n",
    "if torch.cuda.is_available():\n",
    "    label_smoothing_weights = label_smoothing_weights.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdcf2da",
   "metadata": {
    "papermill": {
     "duration": 0.005474,
     "end_time": "2022-12-11T23:17:12.023782",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.018308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating stratified splits for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca3c5922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:17:12.036587Z",
     "iopub.status.busy": "2022-12-11T23:17:12.035741Z",
     "iopub.status.idle": "2022-12-11T23:17:12.167059Z",
     "shell.execute_reply": "2022-12-11T23:17:12.166062Z"
    },
    "papermill": {
     "duration": 0.140411,
     "end_time": "2022-12-11T23:17:12.169705",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.029294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(f'{DATA_PATH}/train.csv')\n",
    "patient_id_any_cancer = train_csv.groupby('patient_id').cancer.max().reset_index()\n",
    "skf = StratifiedKFold(NUM_SPLITS, shuffle=True, random_state=42)\n",
    "splits = list(skf.split(patient_id_any_cancer.patient_id, patient_id_any_cancer.cancer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf007ad",
   "metadata": {
    "papermill": {
     "duration": 0.005815,
     "end_time": "2022-12-11T23:17:12.182599",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.176784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Defining some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cced5b",
   "metadata": {
    "papermill": {
     "duration": 0.005317,
     "end_time": "2022-12-11T23:17:12.193513",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.188196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I am defining some functionality here to make our life easier and to get us the last 10% of the way to a really good result.\n",
    "\n",
    "Generally, none of this code is core to training or predicting, we could skip most of it and still be able to get a well trained model.\n",
    "\n",
    "\n",
    "But here we want to push the boundaries of performance so let's get these things in üôÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd8b65d",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-12-11T23:17:12.206337Z",
     "iopub.status.busy": "2022-12-11T23:17:12.206034Z",
     "iopub.status.idle": "2022-12-11T23:17:12.267726Z",
     "shell.execute_reply": "2022-12-11T23:17:12.266812Z"
    },
    "papermill": {
     "duration": 0.070613,
     "end_time": "2022-12-11T23:17:12.269689",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.199076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/369267  \n",
    "def pfbeta_torch(preds, labels, beta=1):\n",
    "    if preds.dim() != 2 or (preds.dim() == 2 and preds.shape[1] !=2): raise ValueError('Houston, we got a problem')\n",
    "    preds = preds[:, 1]\n",
    "    preds = preds.clip(0, 1)\n",
    "    y_true_count = labels.sum()\n",
    "    ctp = preds[labels==1].sum()\n",
    "    cfp = preds[labels==0].sum()\n",
    "    beta_squared = beta * beta\n",
    "    c_precision = ctp / (ctp + cfp)\n",
    "    c_recall = ctp / y_true_count\n",
    "    if (c_precision > 0 and c_recall > 0):\n",
    "        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n",
    "        return result\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/369886    \n",
    "def pfbeta_torch_thresh(preds, labels):\n",
    "    optimized_preds = optimize_preds(preds, labels)\n",
    "    return pfbeta_torch(optimized_preds, labels)\n",
    "\n",
    "def optimize_preds(preds, labels=None, thresh=None, return_thresh=False, print_results=False):\n",
    "    preds = preds.clone()\n",
    "    if labels is not None: without_thresh = pfbeta_torch(preds, labels)\n",
    "    \n",
    "    if not thresh and labels is not None:\n",
    "        threshs = np.linspace(0, 1, 101)\n",
    "        f1s = [pfbeta_torch((preds > thr).float(), labels) for thr in threshs]\n",
    "        idx = np.argmax(f1s)\n",
    "        thresh, best_pfbeta = threshs[idx], f1s[idx]\n",
    "\n",
    "    preds = (preds > thresh).float()\n",
    "\n",
    "    if print_results:\n",
    "        print(f'without optimization: {without_thresh}')\n",
    "        pfbeta = pfbeta_torch(preds, labels)\n",
    "        print(f'with optimization: {pfbeta}')\n",
    "        print(f'best_thresh = {thresh}')\n",
    "    if return_thresh:\n",
    "        return thresh\n",
    "    return preds\n",
    "\n",
    "fn2label = {fn: cancer_or_not for fn, cancer_or_not in zip(train_csv['image_id'].astype('str'), train_csv['cancer'])}\n",
    "\n",
    "def splitting_func(paths):\n",
    "    train = []\n",
    "    valid = []\n",
    "    for idx, path in enumerate(paths):\n",
    "        if int(path.parent.name) in patient_id_any_cancer.iloc[splits[SPLIT][0]].patient_id.values:\n",
    "            train.append(idx)\n",
    "        else:\n",
    "            valid.append(idx)\n",
    "    return train, valid\n",
    "\n",
    "def label_func(path):\n",
    "    return fn2label[path.stem]\n",
    "\n",
    "def get_items(image_dir_path):\n",
    "    items = []\n",
    "    for p in get_image_files(image_dir_path):\n",
    "        items.append(p)\n",
    "        if p.stem in fn2label and int(p.parent.name) in patient_id_any_cancer.iloc[splits[SPLIT][0]].patient_id.values:\n",
    "            if label_func(p) == 1:\n",
    "                for _ in range(5):\n",
    "                    items.append(p)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6fab9",
   "metadata": {
    "papermill": {
     "duration": 0.005327,
     "end_time": "2022-12-11T23:17:12.280715",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.275388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Wrapping getting data and getting a model into functions -- this way our logic for training will be cleaner to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9305afdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:17:12.293066Z",
     "iopub.status.busy": "2022-12-11T23:17:12.292764Z",
     "iopub.status.idle": "2022-12-11T23:17:12.300332Z",
     "shell.execute_reply": "2022-12-11T23:17:12.299343Z"
    },
    "papermill": {
     "duration": 0.016358,
     "end_time": "2022-12-11T23:17:12.302564",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.286206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from timm.models.layers.adaptive_avgmax_pool import SelectAdaptivePool2d\n",
    "from torch.nn import Flatten\n",
    "\n",
    "def get_dataloaders():\n",
    "    train_image_path = TRAIN_IMAGE_DIR\n",
    "\n",
    "    dblock = DataBlock(\n",
    "        blocks    = (ImageBlock, CategoryBlock),\n",
    "        get_items = get_items,\n",
    "        get_y = label_func,\n",
    "        splitter  = splitting_func,\n",
    "        batch_tfms=[Flip()],\n",
    "    )\n",
    "    dsets = dblock.datasets(train_image_path)\n",
    "    return dblock.dataloaders(train_image_path, batch_size=32)\n",
    "\n",
    "def get_learner(arch=resnet18):\n",
    "    learner = vision_learner(\n",
    "        get_dataloaders(),\n",
    "        arch,\n",
    "        custom_head=nn.Sequential(SelectAdaptivePool2d(pool_type='avg', flatten=Flatten()), nn.Linear(1280, 2)),\n",
    "        metrics=[\n",
    "            error_rate,\n",
    "            AccumMetric(pfbeta_torch, activation=ActivationType.Softmax, flatten=False),\n",
    "            AccumMetric(pfbeta_torch_thresh, activation=ActivationType.Softmax, flatten=False)\n",
    "        ],\n",
    "        loss_func=CrossEntropyLossFlat(weight=torch.tensor([1,50]).float()),\n",
    "        pretrained=True,\n",
    "        normalize=False\n",
    "    ).to_fp16()\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c5143",
   "metadata": {
    "papermill": {
     "duration": 0.005319,
     "end_time": "2022-12-11T23:17:12.313292",
     "exception": false,
     "start_time": "2022-12-11T23:17:12.307973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating the learner and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45def01d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:19:02.469086Z",
     "iopub.status.busy": "2022-12-11T23:19:02.468744Z",
     "iopub.status.idle": "2022-12-11T23:21:06.522875Z",
     "shell.execute_reply": "2022-12-11T23:21:06.521668Z"
    },
    "papermill": {
     "duration": 124.064883,
     "end_time": "2022-12-11T23:21:06.525622",
     "exception": false,
     "start_time": "2022-12-11T23:19:02.460739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'images_1024'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m\n",
      "Cell \u001b[0;32mIn [11], line 19\u001b[0m, in \u001b[0;36mget_learner\u001b[0;34m(arch)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_learner\u001b[39m(arch\u001b[38;5;241m=\u001b[39mresnet18):\n\u001b[1;32m     18\u001b[0m     learner \u001b[38;5;241m=\u001b[39m vision_learner(\n\u001b[0;32m---> 19\u001b[0m         \u001b[43mget_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     20\u001b[0m         arch,\n\u001b[1;32m     21\u001b[0m         custom_head\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(SelectAdaptivePool2d(pool_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m, flatten\u001b[38;5;241m=\u001b[39mFlatten()), nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1280\u001b[39m, \u001b[38;5;241m2\u001b[39m)),\n\u001b[1;32m     22\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     23\u001b[0m             error_rate,\n\u001b[1;32m     24\u001b[0m             AccumMetric(pfbeta_torch, activation\u001b[38;5;241m=\u001b[39mActivationType\u001b[38;5;241m.\u001b[39mSoftmax, flatten\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     25\u001b[0m             AccumMetric(pfbeta_torch_thresh, activation\u001b[38;5;241m=\u001b[39mActivationType\u001b[38;5;241m.\u001b[39mSoftmax, flatten\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m         ],\n\u001b[1;32m     27\u001b[0m         loss_func\u001b[38;5;241m=\u001b[39mCrossEntropyLossFlat(weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m50\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat()),\n\u001b[1;32m     28\u001b[0m         pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m         normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     )\u001b[38;5;241m.\u001b[39mto_fp16()\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m learner\n",
      "Cell \u001b[0;32mIn [11], line 14\u001b[0m, in \u001b[0;36mget_dataloaders\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m train_image_path \u001b[38;5;241m=\u001b[39m TRAIN_IMAGE_DIR\n\u001b[1;32m      7\u001b[0m dblock \u001b[38;5;241m=\u001b[39m DataBlock(\n\u001b[1;32m      8\u001b[0m     blocks    \u001b[38;5;241m=\u001b[39m (ImageBlock, CategoryBlock),\n\u001b[1;32m      9\u001b[0m     get_items \u001b[38;5;241m=\u001b[39m get_items,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     batch_tfms\u001b[38;5;241m=\u001b[39m[Flip()],\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m dsets \u001b[38;5;241m=\u001b[39m \u001b[43mdblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_image_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dblock\u001b[38;5;241m.\u001b[39mdataloaders(train_image_path, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fastai/data/block.py:145\u001b[0m, in \u001b[0;36mDataBlock.datasets\u001b[0;34m(self, source, verbose)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource \u001b[38;5;241m=\u001b[39m source                     ; pv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollecting items from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, verbose)\n\u001b[1;32m    144\u001b[0m items \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_items \u001b[38;5;129;01mor\u001b[39;00m noop)(source) ; pv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(items)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m items\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose)\n\u001b[0;32m--> 145\u001b[0m splits \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mRandomSplitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m pv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(splits)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m datasets of sizes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(s)) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m splits])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, verbose)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Datasets(items, tfms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_type_tfms(), splits\u001b[38;5;241m=\u001b[39msplits, dl_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl_type, n_inp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_inp, verbose\u001b[38;5;241m=\u001b[39mverbose)\n",
      "Cell \u001b[0;32mIn [10], line 50\u001b[0m, in \u001b[0;36msplitting_func\u001b[0;34m(paths)\u001b[0m\n\u001b[1;32m     48\u001b[0m valid \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(paths):\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01min\u001b[39;00m patient_id_any_cancer\u001b[38;5;241m.\u001b[39miloc[splits[SPLIT][\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mpatient_id\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[1;32m     51\u001b[0m         train\u001b[38;5;241m.\u001b[39mappend(idx)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'images_1024'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preds, labels = [], []\n",
    "\n",
    "SPLIT = 0 # our learner needs this to construct its dataloaders...\n",
    "learn = get_learner('tf_efficientnetv2_s')\n",
    "\n",
    "# instead of training, to conserve pipeline time, I am uploading models trained locally\n",
    "# uncomment the lines below for training\n",
    "  \n",
    "# for SPLIT in range(NUM_SPLITS):\n",
    "#     learn = get_learner()\n",
    "#     learn.unfreeze()\n",
    "#     learn.fit_one_cycle(NUM_EPOCHS, 1e-4, pct_start=0.1)\n",
    "#     learn.save(f'{MODEL_PATH}/{SPLIT}')\n",
    "        \n",
    "#     output = learn.get_preds()\n",
    "#     preds.append(output[0])\n",
    "#     labels.append(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c802fd20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:21:06.546885Z",
     "iopub.status.busy": "2022-12-11T23:21:06.546537Z",
     "iopub.status.idle": "2022-12-11T23:21:06.551315Z",
     "shell.execute_reply": "2022-12-11T23:21:06.550361Z"
    },
    "papermill": {
     "duration": 0.020109,
     "end_time": "2022-12-11T23:21:06.555793",
     "exception": false,
     "start_time": "2022-12-11T23:21:06.535684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# threshold = optimize_preds(torch.cat(preds), torch.cat(labels), return_thresh=True, print_results=True)\n",
    "threshold = 0.402"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae5369",
   "metadata": {
    "papermill": {
     "duration": 0.009508,
     "end_time": "2022-12-11T23:21:06.575219",
     "exception": false,
     "start_time": "2022-12-11T23:21:06.565711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predicting on test<a id=\"section-two\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5319e4ff",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-12-11T23:21:06.596836Z",
     "iopub.status.busy": "2022-12-11T23:21:06.596500Z",
     "iopub.status.idle": "2022-12-11T23:21:11.542626Z",
     "shell.execute_reply": "2022-12-11T23:21:11.541306Z"
    },
    "papermill": {
     "duration": 4.959966,
     "end_time": "2022-12-11T23:21:11.545524",
     "exception": false,
     "start_time": "2022-12-11T23:21:06.585558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pydicom\n",
    "# from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "import dicomsdl\n",
    "    \n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "import cv2\n",
    "\n",
    "!rm -rf test_resized_{RESIZE_TO[0]}\n",
    "\n",
    "def dicom_file_to_ary(path):\n",
    "    dcm_file = dicomsdl.open(str(path))\n",
    "    data = dcm_file.pixelData()\n",
    "\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "    if dcm_file.getPixelDataInfo()['PhotometricInterpretation'] == \"MONOCHROME1\":\n",
    "        data = 1 - data\n",
    "\n",
    "    data = cv2.resize(data, RESIZE_TO)\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return data\n",
    "\n",
    "directories = list(Path(TEST_DICOM_DIR).iterdir())\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    parent_directory = str(directory_path).split('/')[-1]\n",
    "    !mkdir -p test_resized_{RESIZE_TO[0]}/{parent_directory}\n",
    "    for image_path in directory_path.iterdir():\n",
    "        processed_ary = dicom_file_to_ary(image_path)\n",
    "        cv2.imwrite(\n",
    "            f'test_resized_{RESIZE_TO[0]}/{parent_directory}/{image_path.stem}.png',\n",
    "            processed_ary\n",
    "        )\n",
    "\n",
    "with mp.Pool(mp.cpu_count()) as p:\n",
    "    p.map(process_directory, directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06661e52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:21:11.562063Z",
     "iopub.status.busy": "2022-12-11T23:21:11.560974Z",
     "iopub.status.idle": "2022-12-11T23:21:30.953165Z",
     "shell.execute_reply": "2022-12-11T23:21:30.951702Z"
    },
    "papermill": {
     "duration": 19.402972,
     "end_time": "2022-12-11T23:21:30.955539",
     "exception": false,
     "start_time": "2022-12-11T23:21:11.552567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.61 s, sys: 1.68 s, total: 5.29 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preds_all = []\n",
    "\n",
    "test_dl = learn.dls.test_dl(get_image_files(f'test_resized_{RESIZE_TO[0]}'))\n",
    "for SPLIT in range(NUM_SPLITS):\n",
    "    learn.load(f'{MODEL_PATH}/{SPLIT}')\n",
    "    preds, _ = learn.get_preds(dl=test_dl)\n",
    "    preds_all.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f27e90c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:21:30.972430Z",
     "iopub.status.busy": "2022-12-11T23:21:30.972123Z",
     "iopub.status.idle": "2022-12-11T23:21:30.980760Z",
     "shell.execute_reply": "2022-12-11T23:21:30.979127Z"
    },
    "papermill": {
     "duration": 0.019482,
     "end_time": "2022-12-11T23:21:30.982952",
     "exception": false,
     "start_time": "2022-12-11T23:21:30.963470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = torch.zeros_like(preds_all[0])\n",
    "for pred in preds_all:\n",
    "    preds += pred\n",
    "\n",
    "preds /= NUM_SPLITS\n",
    "\n",
    "\n",
    "preds = optimize_preds(preds, thresh=threshold)\n",
    "image_ids = [path.stem for path in test_dl.items]\n",
    "\n",
    "image_id2pred = defaultdict(lambda: 0)\n",
    "for image_id, pred in zip(image_ids, preds[:, 1]):\n",
    "    image_id2pred[int(image_id)] = pred.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c1a4f",
   "metadata": {
    "papermill": {
     "duration": 0.007116,
     "end_time": "2022-12-11T23:21:30.997643",
     "exception": false,
     "start_time": "2022-12-11T23:21:30.990527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-three\"></a>\n",
    "# Making a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94742f83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:21:31.014774Z",
     "iopub.status.busy": "2022-12-11T23:21:31.013177Z",
     "iopub.status.idle": "2022-12-11T23:21:31.057843Z",
     "shell.execute_reply": "2022-12-11T23:21:31.056560Z"
    },
    "papermill": {
     "duration": 0.057453,
     "end_time": "2022-12-11T23:21:31.062316",
     "exception": false,
     "start_time": "2022-12-11T23:21:31.004863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction_id</th>\n",
       "      <th>cancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10008_L</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10008_R</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prediction_id  cancer\n",
       "0       10008_L     0.0\n",
       "1       10008_R     0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/test.csv')\n",
    "\n",
    "prediction_ids = []\n",
    "preds = []\n",
    "\n",
    "for _, row in test_csv.iterrows():\n",
    "    prediction_ids.append(row.prediction_id)\n",
    "    preds.append(image_id2pred[row.image_id])\n",
    "\n",
    "submission = pd.DataFrame(data={'prediction_id': prediction_ids, 'cancer': preds}).groupby('prediction_id').max().reset_index()\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af61ec96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T23:21:31.095793Z",
     "iopub.status.busy": "2022-12-11T23:21:31.095143Z",
     "iopub.status.idle": "2022-12-11T23:21:31.113112Z",
     "shell.execute_reply": "2022-12-11T23:21:31.107743Z"
    },
    "papermill": {
     "duration": 0.036385,
     "end_time": "2022-12-11T23:21:31.116858",
     "exception": false,
     "start_time": "2022-12-11T23:21:31.080473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9bf46c",
   "metadata": {
    "papermill": {
     "duration": 0.010281,
     "end_time": "2022-12-11T23:21:31.138116",
     "exception": false,
     "start_time": "2022-12-11T23:21:31.127835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And that's it! Thank you very much for reading! üôÇ\n",
    "\n",
    "**If you enjoyed the notebook, please upvote! üôè Thank you, appreciate your support!**\n",
    "\n",
    "Happy Kaggling ü•≥\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 345.439384,
   "end_time": "2022-12-11T23:21:33.927684",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-11T23:15:48.488300",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
